{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":116062,"databundleVersionId":14084779,"sourceType":"competition"},{"sourceId":13439383,"sourceType":"datasetVersion","datasetId":8530287}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install fair-esm -q","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-20T15:56:31.833632Z","iopub.execute_input":"2025-10-20T15:56:31.834515Z","iopub.status.idle":"2025-10-20T15:56:36.519169Z","shell.execute_reply.started":"2025-10-20T15:56:31.834476Z","shell.execute_reply":"2025-10-20T15:56:36.517681Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nimport os\nimport numpy as np\nimport pickle\nfrom pathlib import Path\n\n# Check GPU\nimport torch\nprint(f\"GPU available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T16:01:16.572558Z","iopub.execute_input":"2025-10-20T16:01:16.573090Z","iopub.status.idle":"2025-10-20T16:01:16.580238Z","shell.execute_reply.started":"2025-10-20T16:01:16.573062Z","shell.execute_reply":"2025-10-20T16:01:16.579061Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir -p src/data src/models src/utils\n\n# We'll create minimal versions of required files inline\n# (Alternative: upload as dataset and unzip)\n\nprint(\"‚úì Directories created\")\nprint(\"Next: Copy minimal code for preprocessing, models, and training\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T11:34:20.133403Z","iopub.execute_input":"2025-10-20T11:34:20.134043Z","iopub.status.idle":"2025-10-20T11:34:20.285320Z","shell.execute_reply.started":"2025-10-20T11:34:20.134009Z","shell.execute_reply":"2025-10-20T11:34:20.284545Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preprocessing_code = '''\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport pickle\n\nclass ProteinDataPreprocessor:\n    def __init__(self, data_dir):\n        self.data_dir = Path(data_dir)\n        self.train_dir = self.data_dir / \"Train\"\n        self.go_term_to_idx = {}\n        self.idx_to_go_term = {}\n        self.go_term_to_aspect = {}\n        self.aspect_to_terms = {\"C\": [], \"F\": [], \"P\": []}\n\n    def load_sequences(self, split=\"train\"):\n        if split == \"train\":\n            fasta_file = self.train_dir / \"train_sequences.fasta\"\n        else:\n            fasta_file = self.data_dir / \"Test\" / \"testsuperset.fasta\"\n\n        sequences = {}\n        current_id = None\n        current_seq = []\n\n        with open(fasta_file, \"r\") as f:\n            for line in f:\n                line = line.strip()\n                if line.startswith(\">\"):\n                    if current_id:\n                        sequences[current_id] = \"\".join(current_seq)\n                    current_id = line[1:].split()[0]\n                    if \"|\" in current_id:\n                        current_id = current_id.split(\"|\")[1]\n                    current_seq = []\n                else:\n                    current_seq.append(line)\n            if current_id:\n                sequences[current_id] = \"\".join(current_seq)\n\n        print(f\"Loaded {len(sequences)} {split} sequences\")\n        return sequences\n\n    def load_annotations(self):\n        annotations_file = self.train_dir / \"train_terms.tsv\"\n        df = pd.read_csv(annotations_file, sep=\"\\\\t\")\n        print(f\"Loaded {len(df)} annotations\")\n        return df\n\n    def build_label_encodings(self, annotations_df):\n        unique_terms = sorted(annotations_df[\"term\"].unique())\n        self.go_term_to_idx = {term: idx for idx, term in enumerate(unique_terms)}\n        self.idx_to_go_term = {idx: term for term, idx in self.go_term_to_idx.items()}\n\n        term_aspect_map = annotations_df[[\"term\", \"aspect\"]].drop_duplicates()\n        self.go_term_to_aspect = dict(zip(term_aspect_map[\"term\"], term_aspect_map[\"aspect\"]))\n\n        for term, aspect in self.go_term_to_aspect.items():\n            self.aspect_to_terms[aspect].append(term)\n\n        print(f\"Total GO terms: {len(self.go_term_to_idx)}\")\n\n    def create_multi_label_matrix(self, annotations_df, protein_ids):\n        n_proteins = len(protein_ids)\n        n_terms = len(self.go_term_to_idx)\n        labels = np.zeros((n_proteins, n_terms), dtype=np.float32)\n\n        protein_to_idx = {pid: idx for idx, pid in enumerate(protein_ids)}\n\n        for _, row in annotations_df.iterrows():\n            protein_id = row[\"EntryID\"]\n            term = row[\"term\"]\n\n            if protein_id in protein_to_idx and term in self.go_term_to_idx:\n                protein_idx = protein_to_idx[protein_id]\n                term_idx = self.go_term_to_idx[term]\n                labels[protein_idx, term_idx] = 1.0\n\n        print(f\"Created label matrix: {labels.shape}\")\n        return labels\n\n    def create_aspect_specific_matrices(self, annotations_df, protein_ids):\n        aspect_data = {}\n\n        for aspect in [\"C\", \"F\", \"P\"]:\n            aspect_terms = self.aspect_to_terms[aspect]\n            aspect_term_indices = [self.go_term_to_idx[term] for term in aspect_terms]\n            aspect_idx_map = {global_idx: local_idx\n                            for local_idx, global_idx in enumerate(aspect_term_indices)}\n\n            n_proteins = len(protein_ids)\n            n_aspect_terms = len(aspect_term_indices)\n            labels = np.zeros((n_proteins, n_aspect_terms), dtype=np.float32)\n\n            protein_to_idx = {pid: idx for idx, pid in enumerate(protein_ids)}\n            aspect_annotations = annotations_df[annotations_df[\"aspect\"] == aspect]\n\n            for _, row in aspect_annotations.iterrows():\n                protein_id = row[\"EntryID\"]\n                term = row[\"term\"]\n\n                if protein_id in protein_to_idx and term in self.go_term_to_idx:\n                    protein_idx = protein_to_idx[protein_id]\n                    global_term_idx = self.go_term_to_idx[term]\n                    local_term_idx = aspect_idx_map[global_term_idx]\n                    labels[protein_idx, local_term_idx] = 1.0\n\n            aspect_data[aspect] = (labels, aspect_term_indices)\n            print(f\"Aspect {aspect}: {labels.shape}\")\n\n        return aspect_data\n'''\n\nwith open('/kaggle/working/preprocessingp.py', 'w') as f:\n    f.write(preprocessing_code)\n\nprint(\"‚úì Preprocessing code created\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T11:37:13.813944Z","iopub.execute_input":"2025-10-20T11:37:13.814218Z","iopub.status.idle":"2025-10-20T11:37:13.821106Z","shell.execute_reply.started":"2025-10-20T11:37:13.814199Z","shell.execute_reply":"2025-10-20T11:37:13.820344Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom tqdm import tqdm\nimport esm\nimport pickle\nimport gc\n\n# Configuration\n# MODEL_NAME = 'esm2_t30_150M_UR50D'  # Using 150M for speed\nMODEL_NAME = 'esm2_t6_8M_UR50D'  # Using 8M for speed\nBATCH_SIZE = 16  # Small batch for Kaggle GPU\nMAX_LENGTH = 1024\nDATA_DIR = '/kaggle/input/cafa-6-protein-function-prediction'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T15:56:47.993980Z","iopub.execute_input":"2025-10-20T15:56:47.994349Z","iopub.status.idle":"2025-10-20T15:56:48.000571Z","shell.execute_reply.started":"2025-10-20T15:56:47.994322Z","shell.execute_reply":"2025-10-20T15:56:47.999358Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Truncation helper\ndef truncate_balanced(seq, max_len):\n    if len(seq) <= max_len:\n        return seq\n    n_term = max_len // 2\n    c_term = max_len - n_term\n    return seq[:n_term] + seq[-c_term:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T16:01:24.152147Z","iopub.execute_input":"2025-10-20T16:01:24.152893Z","iopub.status.idle":"2025-10-20T16:01:24.157651Z","shell.execute_reply.started":"2025-10-20T16:01:24.152845Z","shell.execute_reply":"2025-10-20T16:01:24.156704Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Loading ESM-2 model: {MODEL_NAME}\")\nmodel, alphabet = esm.pretrained.load_model_and_alphabet(MODEL_NAME)\nbatch_converter = alphabet.get_batch_converter()\nmodel = model.cuda()\nmodel.eval()\nnum_layers = model.num_layers\n\n# Load sequences\nsys.path.append('/kaggle/working')\nfrom preprocessingp import ProteinDataPreprocessor\n\npreprocessor = ProteinDataPreprocessor(DATA_DIR)\nsequences = preprocessor.load_sequences('train')\nprotein_ids = list(sequences.keys())\n\nprint(f\"\\nGenerating embeddings for {len(protein_ids)} proteins...\")\n\n\n# Generate embeddings in batches\nall_embeddings = []\nall_protein_ids = []\n\nfor i in tqdm(range(0, len(protein_ids), BATCH_SIZE)):\n    batch_ids = protein_ids[i:i + BATCH_SIZE]\n    batch_data = [(pid, truncate_balanced(sequences[pid], MAX_LENGTH))\n                  for pid in batch_ids]\n\n    # Convert and embed\n    _, _, batch_tokens = batch_converter(batch_data)\n    batch_tokens = batch_tokens.cuda()\n\n    with torch.no_grad():\n        results = model(batch_tokens, repr_layers=[num_layers], return_contacts=False)\n        embeddings = results['representations'][num_layers][:, 0, :].cpu().numpy()\n\n    all_embeddings.extend(embeddings)\n    all_protein_ids.extend(batch_ids)\n\n    # Clear GPU memory periodically\n    if (i // BATCH_SIZE) % 100 == 0:\n        torch.cuda.empty_cache()\n        gc.collect()\n\nembeddings_array = np.array(all_embeddings)\n\n# Save embeddings\nembeddings_data = {\n    'embeddings': embeddings_array,\n    'protein_ids': all_protein_ids,\n    'model_name': MODEL_NAME,\n    'embedding_dim': model.embed_dim,\n    'num_proteins': len(all_protein_ids)\n}\n\nwith open('/kaggle/working/train_embeddings.pkl', 'wb') as f:\n    pickle.dump(embeddings_data, f)\n\nprint(f\"\\n‚úì Embeddings generated: {embeddings_array.shape}\")\nprint(f\"‚úì Saved to /kaggle/working/train_embeddings.pkl\")\nprint(f\"Size: {Path('/kaggle/working/train_embeddings.pkl').stat().st_size / 1e9:.2f} GB\")\n\n# Free GPU memory\ndel model\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T11:39:36.193727Z","iopub.execute_input":"2025-10-20T11:39:36.193978Z","iopub.status.idle":"2025-10-20T12:10:34.685758Z","shell.execute_reply.started":"2025-10-20T11:39:36.193960Z","shell.execute_reply":"2025-10-20T12:10:34.685181Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sys.path.append('/kaggle/working')\nfrom preprocessing import ProteinDataPreprocessor\n\npreprocessor = ProteinDataPreprocessor(DATA_DIR)\nsequences = preprocessor.load_sequences('train')\nannotations_df = preprocessor.load_annotations()\npreprocessor.build_label_encodings(annotations_df)\n\nprotein_ids = sorted(list(set(sequences.keys()) & set(annotations_df['EntryID'].unique())))\nprint(f\"Proteins with both sequence and annotations: {len(protein_ids)}\")\n\n# Create label matrices\nfull_labels = preprocessor.create_multi_label_matrix(annotations_df, protein_ids)\naspect_labels = preprocessor.create_aspect_specific_matrices(annotations_df, protein_ids)\n\n# Save\npreprocessed_data = {\n    'protein_ids': protein_ids,\n    'full_labels': full_labels,\n    'aspect_labels': aspect_labels,\n    'go_term_to_idx': preprocessor.go_term_to_idx,\n    'idx_to_go_term': preprocessor.idx_to_go_term,\n    'go_term_to_aspect': preprocessor.go_term_to_aspect\n}\n\nwith open('/kaggle/working/preprocessed_labels.pkl', 'wb') as f:\n    pickle.dump(preprocessed_data, f)\n\nprint(\"‚úì Labels preprocessed and saved\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T12:55:14.966972Z","iopub.execute_input":"2025-10-20T12:55:14.967495Z","iopub.status.idle":"2025-10-20T12:57:06.974430Z","shell.execute_reply.started":"2025-10-20T12:55:14.967471Z","shell.execute_reply":"2025-10-20T12:57:06.973609Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\n# Simple model\nclass SimpleClassifier(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super().__init__()\n        self.classifier = nn.Sequential(\n            nn.Linear(input_dim, 1024),\n            nn.ReLU(),\n            nn.BatchNorm1d(1024),\n            nn.Dropout(0.3),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.BatchNorm1d(512),\n            nn.Dropout(0.3),\n            nn.Linear(512, num_classes)\n        )\n\n    def forward(self, x):\n        return self.classifier(x)\n\n# Simple dataset\nclass EmbeddingDataset(Dataset):\n    def __init__(self, embeddings, labels):\n        self.embeddings = torch.FloatTensor(embeddings)\n        self.labels = torch.FloatTensor(labels)\n\n    def __len__(self):\n        return len(self.embeddings)\n\n    def __getitem__(self, idx):\n        return {\n            'embedding': self.embeddings[idx],\n            'labels': self.labels[idx]\n        }\n\n# Focal Loss (simplified)\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.25, gamma=2.0):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def forward(self, inputs, targets):\n        bce_loss = nn.functional.binary_cross_entropy_with_logits(\n            inputs, targets, reduction='none'\n        )\n        probs = torch.sigmoid(inputs)\n        p_t = probs * targets + (1 - probs) * (1 - targets)\n        focal_weight = (1 - p_t) ** self.gamma\n        alpha_weight = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n        loss = alpha_weight * focal_weight * bce_loss\n        return loss.mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T16:01:30.517651Z","iopub.execute_input":"2025-10-20T16:01:30.518430Z","iopub.status.idle":"2025-10-20T16:01:30.533266Z","shell.execute_reply.started":"2025-10-20T16:01:30.518395Z","shell.execute_reply":"2025-10-20T16:01:30.532111Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load data\nwith open('/kaggle/working/train_embeddings.pkl', 'rb') as f:\n    emb_data = pickle.load(f)\nwith open('/kaggle/working/preprocessed_labels.pkl', 'rb') as f:\n    label_data = pickle.load(f)\n\nembeddings = emb_data['embeddings']\nlabels = label_data['full_labels']\n\nprint(f\"Embeddings: {embeddings.shape}\")\nprint(f\"Labels: {labels.shape}\")\n\n# Create dataset\ndataset = EmbeddingDataset(embeddings, labels)\ntrain_size = int(0.9 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\n# Model\nmodel = SimpleClassifier(\n    input_dim=emb_data['embedding_dim'],\n    num_classes=labels.shape[1]\n).cuda()\n\ncriterion = FocalLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\nprint(f\"\\n{'='*60}\")\nprint(\"Training baseline model...\")\nprint(f\"{'='*60}\\n\")\n\n# Training loop\nfor epoch in range(10):\n    # Train\n    model.train()\n    train_loss = 0\n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/10\"):\n        embeddings = batch['embedding'].cuda()\n        labels_batch = batch['labels'].cuda()\n\n        optimizer.zero_grad()\n        outputs = model(embeddings)\n        loss = criterion(outputs, labels_batch)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n\n    # Validate\n    model.eval()\n    val_loss = 0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch in val_loader:\n            embeddings = batch['embedding'].cuda()\n            labels_batch = batch['labels'].cuda()\n\n            outputs = model(embeddings)\n            loss = criterion(outputs, labels_batch)\n            val_loss += loss.item()\n\n            preds = torch.sigmoid(outputs).cpu().numpy()\n            all_preds.append(preds)\n            all_labels.append(labels_batch.cpu().numpy())\n\n    # Calculate F1\n    all_preds = np.vstack(all_preds)\n    all_labels = np.vstack(all_labels)\n\n    from sklearn.metrics import f1_score\n    f1 = f1_score(all_labels, (all_preds > 0.5).astype(int),\n                  average='micro', zero_division=0)\n\n    print(f\"Epoch {epoch+1}: Train Loss={train_loss/len(train_loader):.4f}, \"\n          f\"Val Loss={val_loss/len(val_loader):.4f}, F1={f1:.4f}\")\n\n# Save model\ntorch.save({\n    'model_state_dict': model.state_dict(),\n    'embedding_dim': emb_data['embedding_dim'],\n    'num_classes': labels.shape[1]\n}, '/kaggle/working/baseline_model.pt')\n\nprint(f\"\\n‚úì Model trained and saved!\")\nprint(f\"Final F1: {f1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T14:30:08.098354Z","iopub.execute_input":"2025-10-20T14:30:08.098659Z","iopub.status.idle":"2025-10-20T14:31:14.741572Z","shell.execute_reply.started":"2025-10-20T14:30:08.098638Z","shell.execute_reply":"2025-10-20T14:31:14.740287Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport esm\nimport pickle\nimport gc\nfrom tqdm import tqdm\nimport sys\n\nprint(\"Loading ESM-2 model...\")\nMODEL_NAME = 'esm2_t6_8M_UR50D'  # Using 8M for speed\nBATCH_SIZE = 16  # Small batch for Kaggle GPU\nMAX_LENGTH = 1024\nprint(f\"{MODEL_NAME}\")\nmodel, alphabet = esm.pretrained.load_model_and_alphabet(MODEL_NAME)\nbatch_converter = alphabet.get_batch_converter()\nmodel = model.cuda()\nmodel.eval()\nnum_layers = model.num_layers\n\n# Load test sequences\nprint(\"Loading test sequences...\")\nsys.path.append('/kaggle/working')\nfrom preprocessing import ProteinDataPreprocessor\n\npreprocessor = ProteinDataPreprocessor(DATA_DIR)\ntest_sequences = preprocessor.load_sequences('test')\ntest_protein_ids = list(test_sequences.keys())\n\nprint(f\"Generating embeddings for {len(test_protein_ids)} test proteins...\")\n\n# Generate embeddings\ntest_embeddings = []\ntest_ids = []\n\nfor i in tqdm(range(0, len(test_protein_ids), BATCH_SIZE)):\n    batch_ids = test_protein_ids[i:i + BATCH_SIZE]\n    batch_data = [(pid, truncate_balanced(test_sequences[pid], MAX_LENGTH))\n                  for pid in batch_ids]\n\n    _, _, batch_tokens = batch_converter(batch_data)\n    batch_tokens = batch_tokens.cuda()\n\n    with torch.no_grad():\n        results = model(batch_tokens, repr_layers=[num_layers], return_contacts=False)\n        embeddings = results['representations'][num_layers][:, 0, :].cpu().numpy()\n\n    test_embeddings.extend(embeddings)\n    test_ids.extend(batch_ids)\n\n    if (i // BATCH_SIZE) % 100 == 0:\n        torch.cuda.empty_cache()\n        gc.collect()\n\ntest_embeddings_array = np.array(test_embeddings)\n\n# Save test embeddings\ntest_emb_data = {\n    'embeddings': test_embeddings_array,\n    'protein_ids': test_ids,\n    'model_name': MODEL_NAME,\n    'embedding_dim': model.embed_dim\n}\n\nwith open('/kaggle/working/test_embeddings.pkl', 'wb') as f:\n    pickle.dump(test_emb_data, f)\n\nprint(f\"\\n‚úì Test embeddings generated: {test_embeddings_array.shape}\")\n\n# Free memory\ndel model\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T13:09:19.569734Z","iopub.execute_input":"2025-10-20T13:09:19.570007Z","iopub.status.idle":"2025-10-20T14:20:04.233473Z","shell.execute_reply.started":"2025-10-20T13:09:19.569989Z","shell.execute_reply":"2025-10-20T14:20:04.232899Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Kaggle Notebook Cell - Memory-Efficient Submission Creation (TSV format)\n\nimport torch\nimport numpy as np\nimport pandas as pd\nimport pickle\nfrom tqdm import tqdm\nimport gc\n\nprint(\"Loading data...\")\n\n# Load test embeddings\nwith open('/kaggle/working/test_embeddings.pkl', 'rb') as f:\n    test_data = pickle.load(f)\ntest_embeddings = test_data['embeddings']\nprotein_ids = test_data['protein_ids']\n\n# Load model\ncheckpoint = torch.load('/kaggle/working/baseline_model.pt')\nmodel = SimpleClassifier(\n    input_dim=checkpoint['embedding_dim'],\n    num_classes=checkpoint['num_classes']\n).cuda()\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\n\n# Load GO term mappings\nwith open('/kaggle/working/preprocessed_labels.pkl', 'rb') as f:\n    label_data = pickle.load(f)\nidx_to_go_term = label_data['idx_to_go_term']\ndel f\n\nprint(f\"Generating predictions for {len(protein_ids)} proteins...\")\n\n# ============================================================\n# Memory-Efficient Submission Creation (TSV format)\n# ============================================================\n\noutput_file = '/kaggle/working/submission.tsv'\nthreshold = 0.5\nbatch_size = 64  # Small batch to avoid OOM\n\n# Write header (TSV format - tab separated)\nwith open(output_file, 'w') as f:\n    f.write(\"Protein ID\\tGO Term\\tConfidence\\n\")\n\ntotal_preds = 0\nno_pred_count = 0\n\n# Process in batches and write immediately\nwith torch.no_grad():\n    for start_idx in tqdm(range(0, len(protein_ids), batch_size)):\n        end_idx = min(start_idx + batch_size, len(protein_ids))\n\n        # Get batch\n        batch_emb = test_embeddings[start_idx:end_idx]\n        batch_ids = protein_ids[start_idx:end_idx]\n\n        # Predict\n        batch_tensor = torch.FloatTensor(batch_emb).cuda()\n        outputs = model(batch_tensor)\n        probs = torch.sigmoid(outputs).cpu().numpy()\n\n        # Process each protein in batch\n        rows = []\n        for i, pid in enumerate(batch_ids):\n            protein_probs = probs[i]\n\n            # Get predictions above threshold\n            pred_indices = np.where(protein_probs > threshold)[0]\n\n            # If no predictions, use top 3\n            if len(pred_indices) == 0:\n                no_pred_count += 1\n                pred_indices = np.argsort(protein_probs)[-3:]\n\n            # Create TSV rows for this protein (tab separated)\n            for idx in pred_indices:\n                go_term = idx_to_go_term[idx]\n                confidence = float(protein_probs[idx])\n                rows.append(f\"{pid}\\t{go_term}\\t{confidence:.6f}\\n\")\n                total_preds += 1\n\n        # Write batch to file immediately\n        with open(output_file, 'a') as f:\n            f.writelines(rows)\n\n        # Clear memory aggressively\n        del batch_tensor, outputs, probs, rows\n        gc.collect()\n        torch.cuda.empty_cache()\n\nprint(f\"\\n{'='*60}\")\nprint(f\"‚úì Submission created: {output_file}\")\nprint(f\"  Total predictions: {total_preds}\")\nprint(f\"  Proteins with no predictions: {no_pred_count}\")\nprint(f\"{'='*60}\")\n\n# Validate (TSV format - tab separated)\nsubmission = pd.read_csv(output_file, sep='\\t', nrows=20)\nprint(f\"\\nFirst 20 rows:\")\nprint(submission)\n\n# Check stats\nsubmission_full = pd.read_csv(output_file, sep='\\t')\nprint(f\"\\nSubmission stats:\")\nprint(f\"  Total rows: {len(submission_full)}\")\nprint(f\"  Unique proteins: {submission_full['Protein ID'].nunique()}\")\nprint(f\"  Unique GO terms: {submission_full['GO Term'].nunique()}\")\n\n# Verify all proteins have predictions\nsubmitted_proteins = submission_full['Protein ID'].nunique()\nif submitted_proteins < len(protein_ids):\n    print(f\"‚ö† Warning: {len(protein_ids) - submitted_proteins} proteins missing!\")\nelse:\n    print(f\"‚úì All {len(protein_ids)} proteins have predictions\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T16:01:43.202125Z","iopub.execute_input":"2025-10-20T16:01:43.202889Z","iopub.status.idle":"2025-10-20T16:11:11.629379Z","shell.execute_reply.started":"2025-10-20T16:01:43.202857Z","shell.execute_reply":"2025-10-20T16:11:11.628429Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def validate_submission(submission_df, test_protein_ids):\n    \"\"\"Validate submission meets competition requirements\"\"\"\n\n    issues = []\n\n    # Check required columns\n    required_cols = ['Protein ID', 'GO Term', 'Confidence']\n    if not all(col in submission_df.columns for col in required_cols):\n        issues.append(f\"Missing required columns. Need: {required_cols}\")\n\n    # Check all test proteins have predictions\n    submission_proteins = set(submission_df['Protein ID'].unique())\n    missing_proteins = set(test_protein_ids) - submission_proteins\n    if missing_proteins:\n        issues.append(f\"Missing predictions for {len(missing_proteins)} proteins\")\n        print(f\"  First 5 missing: {list(missing_proteins)[:5]}\")\n\n    # Check confidence scores are in [0, 1]\n    if submission_df['Confidence'].min() < 0 or submission_df['Confidence'].max() > 1:\n        issues.append(\"Confidence scores must be in [0, 1]\")\n\n    # Check GO term format\n    invalid_terms = submission_df[~submission_df['GO Term'].str.match(r'^GO:\\d{7}$')]\n    if len(invalid_terms) > 0:\n        issues.append(f\"Found {len(invalid_terms)} invalid GO term formats\")\n\n    # Check for NaN values\n    if submission_df.isnull().any().any():\n        issues.append(\"Found NaN values in submission\")\n\n    # Statistics\n    print(\"Submission Statistics:\")\n    print(f\"  Total predictions: {len(submission_df):,}\")\n    print(f\"  Unique proteins: {submission_df['Protein ID'].nunique():,}\")\n    print(f\"  Unique GO terms: {submission_df['GO Term'].nunique():,}\")\n    print(f\"  Avg predictions per protein: {len(submission_df) / submission_df['Protein ID'].nunique():.1f}\")\n    print(f\"  Min confidence: {submission_df['Confidence'].min():.4f}\")\n    print(f\"  Max confidence: {submission_df['Confidence'].max():.4f}\")\n    print(f\"  Avg confidence: {submission_df['Confidence'].mean():.4f}\")\n\n    if issues:\n        print(\"\\n‚ùå Validation Issues:\")\n        for issue in issues:\n            print(f\"  - {issue}\")\n        return False\n    else:\n        print(\"\\n‚úÖ Submission is valid!\")\n        return True\n\nsubmission_df = submission_full\nwith open('/kaggle/working/test_embeddings.pkl', 'rb') as f:\n    test_data = pickle.load(f)\n\n# print(f\"{test_data['protein_ids']}\")\nprint(f\"{submission_df.columns}\")\n# Validate\nis_valid = validate_submission(submission_df, test_data['protein_ids'])\n\nif is_valid:\n    print(\"\\nüéâ Ready to submit to Kaggle!\")\nelse:\n    print(\"\\n‚ö†Ô∏è Please fix issues before submitting\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T16:28:51.993704Z","iopub.execute_input":"2025-10-20T16:28:51.994077Z","iopub.status.idle":"2025-10-20T16:28:52.693046Z","shell.execute_reply.started":"2025-10-20T16:28:51.994042Z","shell.execute_reply":"2025-10-20T16:28:52.692045Z"}},"outputs":[],"execution_count":null}]}